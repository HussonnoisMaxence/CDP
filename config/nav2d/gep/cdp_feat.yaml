#Environment

Environment:
  name : 'Nav2d'
  type : '2d'
  config :
    goal : [200,200]
    
Training:
  seed : 2
  step_max : 100
  k : 20
  prior : False
  exp: 'TR_2d_nav'
  learn_reward: True
  track_metric : 90000000000000000
  use_vq : True
  Exploration :
    scale_factors : [1,1, 1 ] #[0.5,0.3,0.2]
    total_timesteps:  50_000
    step_max : 100
    random_sample : 2_000
    start_guide : 5_000
    update_guide : 5_000
    update_reward : 5_000
    update_after_reset: 100
    diversity: 'reverse'

  Focus:
    pref_embedding: 'state'
    reward_update : 200
    max_feedback : 1400
    rts: 0.7

  Discovery: 
    div_embedding : 'next_state'
    batch_size: 256
    learning_rate: 1.0e-3
    epoch: 1000

  Learning:
    total_timesteps: 100_000
    step_max: 100
    learning_start :  10_000
  
SMM:
  sp_lr: 1.0e-3
  vae_lr: 1.0e-2
  hidden_dim : 128
  vae_beta: 0.5


Logger:
  log_dir: './Projects/CDP/experiments/nav2d/test/cdp/gep/'
  save_tb: True
  log_frequency: 50_000
  file_name: 'cdp'
  agent: 'cdp'
  save_data : False



Discriminator :
  codebook_size: 10
  code_size: 10
  beta: 0.3
  normalize_inputs: True
  hidden_size: 32
  num_layers: 3


Reward_Model:
  name : 'pairwise-state'
  net_arch: [256,256] 
  segment_size: 50
  batch_size: 128
  query_number: 140
  ensemble_size: 3
  learning_rate: 3.0e-4
  reward_update : 200
  sampling_mode : 'uniform'
  reward_mode : 'uni'
  pref_mode : ''
  memory_capacity: 1_000_000
  hidden_size: [256,256,16]
  num_layers: 3
SAC_SMM :  
  #Soft-Actor-Critic config
  gamma: 0.99
  capacity: 10_000_000
  polyak_factor: 0.01
  batch_size: 256
  train_freq: 1 #5000
  gradient_steps: 1 #8
  alpha_auto : True
  alpha : 0.2

  actor_train_freq : 1
  critic_train_freq : 1
  ##Networksx
  learning_rate: 3.0e-3
  policy_kwargs:  {log_std_bound: [-5,2], hidden_dim: 300, hidden_depth : 2 }
  betas: [0.99,0.999]
  use_sde: False

SAC :  
  #Soft-Actor-Critic config
  gamma: 0.99
  capacity: 10_000_000
  polyak_factor: 0.001
  batch_size: 256 #128
  train_freq: 1 #5000
  gradient_steps: 1 #8
  alpha_auto : True
  alpha : 1

  actor_train_freq : 1
  critic_train_freq : 1
  ##Networksx
  learning_rate: 3.0e-4
  policy_kwargs:  {log_std_bound: [-5,2], hidden_dim: 300, hidden_depth : 2 }
  betas: [0.99,0.999]
  use_sde: False
  