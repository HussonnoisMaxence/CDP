#Environment

Environment:
  name : 'HC'
  type : 'MJ'
  config :
    Task: 'backward'
    model_path: 'half_cheetah.xml'
    eval_path: 'half_cheetahT.xml'
Training:
  seed : 2
  step_max : 100
  k : 20
  prior : True
  learn_reward: True
  track_metric : 1000000000000
  use_vq : True
  Exploration :
    scale_factors : [1,1, 1 ]
    total_timesteps:  1_000_000
    random_sample : 10_000
    start_guide : 50_000
    update_guide : 50_000
    update_reward : 50_000
    update_after_reset: 100
    diversity: 'reverse'

  Focus:
    pref_embedding: 'state'
    reward_update : 200
    max_feedback : 1400
    rts: 0.3
  Discovery:
    div_embedding : 'reward_feature'
    batch_size: 256
    learning_rate: 1.0e-4
    epoch: 2000

  Learning:
    total_timesteps: 1_000_000
    step_max: 100
    learning_start :  10_000
  
SMM:
  sp_lr: 1.0e-3
  vae_lr: 1.0e-2
  hidden_dim : 128
  vae_beta: 0.5

Logger:
  log_dir: './Projects/CDP/experiments/Mujoco/HC/'
  save_tb: True
  log_frequency: 10000
  file_name: 'results'
  agent: 'cdp'
  comments : ''
  save_data : False


Discriminator :
  codebook_size: 10
  code_size: 10
  beta: 0.5
  normalize_inputs: True
  hidden_size: 128
  num_layers: 3


Reward_Model:
  name : 'pairwise-state'
  net_arch: [256,256] 
  segment_size: 50
  batch_size: 128
  query_number: 140
  ensemble_size: 3
  learning_rate: 3.0e-4
  reward_update : 200
  sampling_mode : 'uniform'
  reward_mode : 'uni'
  pref_mode : ''
  memory_capacity: 1_000_000
  hidden_size: [256,256,6]
  num_layers: 3
  
SAC_SMM:  
  #Soft-Actor-Critic config
  gamma: 0.99
  capacity: 10_000_000
  polyak_factor: 0.01
  batch_size: 256
  train_freq: 1 #5000
  gradient_steps: 1 #8
  alpha_auto : True
  alpha : 1

  actor_train_freq : 1
  critic_train_freq : 1
  ##Networksx
  learning_rate: 3.0e-3
  policy_kwargs:  {log_std_bound: [-5,2], hidden_dim: 300, hidden_depth : 2 }
  betas: [0.99,0.999]
  use_sde: False

SAC:  
  #Soft-Actor-Critic config
  gamma: 0.99
  capacity: 10_000_000
  polyak_factor: 0.001
  batch_size: 256 #128
  train_freq: 1 #5000
  gradient_steps: 1 #8
  alpha_auto : True
  alpha : 1

  actor_train_freq : 1
  critic_train_freq : 1
  ##Networksx
  learning_rate: 3.0e-4
  policy_kwargs:  {log_std_bound: [-5,2], hidden_dim: 300, hidden_depth : 2 }
  betas: [0.99,0.999]
  use_sde: False
  
  